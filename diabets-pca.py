# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m-0CslCY5Qzl_8vCmaYKTRJTdBoMwQAd

# Import Library
Import pandas library for data input, matplotlib for plotting data to graphs, numpy for calculations, and scikit-learn for PCA calculations
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# %matplotlib inline

"""# Upload data ke Google Collab
In this case, upload the predetermined datasheet (.xlsx), namely the diabetes data that has been edited so that it can be read by the library.
"""

from google.colab import files
uploaded = files.upload()

"""The data that has been uploaded will have a display like the following, which is the same as the data in Excel."""

import io
df = pd.read_excel(io.BytesIO(uploaded['dataDiabetes-edited.xlsx']))
df

"""# Standardize data
PCA is scale-influenced so it is necessary to scale features in the data before implementing PCA. In this case I am using StandardScaler to help standardize the feature set data to a unit scale (mean = 0 and variance = 1) which is a requirement for getting optimal performance from many machine learning algorithms.
"""

features = ["preg", "plas", "pres", "skin","insu", "mass","pedi", "age"]
x = df.loc[:, features].values

y = df.loc[:,['test']].values

x = StandardScaler().fit_transform(x)

"""The data display after standardization will be like the following table."""

pd.DataFrame(data = x, columns = features).head()

"""# PC projection to 7D
The original data has 8 columns (preg, plas, press, skin, insu, mass, pedi, and age). In this section the code projects the original data measuring 8 dimensions into 7 dimensions. Note that after dimensionality reduction, usually no specific meaning is assigned to each major component. The new components are just the 7 main dimensions of variation.
"""

pca = PCA(n_components=7)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7'])

"""The PCA calculation results are shown in the following table.
Here it only displays 5 rows of heads for easy visualization
"""

principalDf.head(5)

"""The target we will analyze the PCA for is the y (test) component which consists of 2 components, namely tested_positive and tested_negative"""

df[['test']].head()

"""# Visualization of combined PCA and target data
Concatenate a DataFrame along axis = 1. finalDf is the last DataFrame before plotting data.
"""

finalDf = pd.concat([principalDf, df[['test']]], axis = 1)
finalDf.head(5)

"""To get the Scree Plot, you must print the variance of each principal component"""

per_var = np.round(pca.explained_variance_ratio_* 100, decimals=2)
labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]

plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)
plt.ylabel('Percentage of Explained Variance')
plt.xlabel('Principal Component')
plt.title('Scree Plot')
plt.show()

pca.explained_variance_ratio_

"""The variance described indicates that how much information (variance) can be attributed to each of the major components. This is important because although we can convert an 8 dimensional space into a 7 dimensional space it can cause some loss of variance (information) when we do so. Using the explained_varians_rasio_ attribute, we can see that the first principal component contains 25.56% variant, the second principal component contains 21.45% variant, third contains 12.92% variant, fourth contains 11.67% variant, fifth contains 9.5% variant, sixth contains 8.53% variant, and seventh contains 5.29% variant. Together, the seven components contain 94.92% information. So that the remaining 1 main component (eight) contains the remaining information, namely 5.08% information.

# Detailed Information for each PCA
"""

for i in range(0, 7):
    loading_scores = pd.Series(pca.components_[i], index=["preg", "plas", "pres", "skin","insu", "mass","pedi", "age"])
    sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)
    print("PC" + str(i+1))
    print(sorted_loading_scores)
    print("\n")

"""# Conclusion
Based on the PCA results, it shows that using only 7 PCA components has obtained the power of 94.92% of information so there is no need to look for a PC for the remaining data. But in this case, if you use a PC less than 7, the value will be below 90% so the accuracy of the data will be reduced.
"""